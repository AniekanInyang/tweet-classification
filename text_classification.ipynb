{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datasets into pandas dataframes\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See unique values in keyword and location columns to have an idea about the data\n",
    "for col in train_df.columns[1:len(train_df.columns)-2]:\n",
    "    print('{} values of {} column: {} \\n '.format(len(train_df[col].value_counts()), col, train_df[col].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "'''\n",
    "STEMMING:\n",
    "PorterStemmer and SnowballStemmer give similar results but Porter stemmer is an older algorithm.\n",
    "It’s from the 1980s and its main concern is removing the common endings to words so that they can be resolved \n",
    "to a common form. Typically, it’s not really advised to use it for any production/complex application. \n",
    "Instead, it has its place in research as a nice, basic stemming algorithm that can guarantee reproducibility. \n",
    "It also is a very gentle stemming algorithm when compared to others.\n",
    "Snowball stemmer is also known as the Porter2 stemming algorithm. \n",
    "It is almost universally accepted as better than the Porter stemmer, \n",
    "even being acknowledged as such by the individual who created the Porter stemmer. \n",
    "That being said, it is also more aggressive than the Porter stemmer.\n",
    "A lot of the things added to the Snowball stemmer were because of issues noticed with the Porter stemmer. \n",
    "There is about a 5% difference in the way that Snowball stems versus Porter.\n",
    "\n",
    "\n",
    "One more thing before I wrap up here: If you choose to use either lemmatization or stemming in your NLP application, \n",
    "always be sure to test performance with that addition. In many applications, \n",
    "you may find that either ends up messing with performance in a bad way just as often as it helps boost performance. \n",
    "Both of these techniques are really designed with recall in mind, but precision tends to suffer as a result. \n",
    "But if recall is what you’re aiming for (like with a search engine) then maybe that’s alright!\n",
    "https://towardsdatascience.com/stemming-lemmatization-what-ba782b7c0bd8\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download list of stopwords (only once; need not run it again)\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "import regex as re\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# Function to preprocess data with NLTK\n",
    "def preprocess_nltk(text):\n",
    "    text = [re.sub(r'[^a-zA-Z0-9]', ' ', text) for text in text]\n",
    "    filtered_sentence = [word for word in text if not word in stop_words]\n",
    "    words = [word_tokenize(PorterStemmer().stem(WordNetLemmatizer(). lemmatize(w, pos='v'))) for w in filtered_sentence]\n",
    "    return words\n",
    "\n",
    "# Function to preprocess data with Gensim\n",
    "def preprocess_gensim(text):\n",
    "    # Remove non-alphanumeric characters from data\n",
    "    text = [re.sub(r'[^a-zA-Z0-9]', ' ', text) for text in text]\n",
    "    \n",
    "    # Lemmatize, stem and tokenize words in the dataset, removing stopwords\n",
    "    text = [(PorterStemmer().stem(WordNetLemmatizer(). lemmatize(w, pos='v')) )for w in text]\n",
    "    result = [[token for token in gensim.utils.simple_preprocess(sentence) if not token in \n",
    "              gensim.parsing.preprocessing.STOPWORDS and len(token) > 3] for sentence in text]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTLK stopwords vs. Gensim stopwords\n",
    "print('NTLK stopwords: {} \\n \\n \\n Gensim stopwords: {}'.format(stop_words, gensim.parsing.preprocessing.STOPWORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Adding more stopwords unique to the data to the WordCloud stopwords list\n",
    "additional_wordcloud_stopwords = ['http', 'https', 'co', 'amp', 'you', 'to', 'us', 'will']\n",
    "\n",
    "#Create a WordCloud of real disaster tweets\n",
    "real_data = train_df[train_df['target'] == 1]\n",
    "combined_real_data = real_data['text'].tolist()\n",
    "combined_real_data = [re.sub(r'[^a-zA-Z0-9]', ' ', text) for text in combined_real_data]\n",
    "combined_real_data = \" \".join([review for review in combined_real_data])\n",
    "\n",
    "wc = WordCloud(background_color='white', max_words=50,\n",
    "              stopwords=STOPWORDS.update(additional_wordcloud_stopwords))\n",
    "\n",
    "plt.imshow(wc.generate(combined_real_data))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a WordCloud of fake disaster tweets\n",
    "fake_data = train_df[train_df['target'] == 0]\n",
    "combined_fake_data = fake_data['text'].tolist()\n",
    "combined_fake_data = [re.sub(r'[^a-zA-Z0-9]', ' ', text) for text in combined_fake_data]\n",
    "combined_fake_data = \" \".join([review for review in combined_fake_data])\n",
    "\n",
    "wc = WordCloud(background_color='white', max_words=50,\n",
    "              stopwords=STOPWORDS.update(additional_wordcloud_stopwords))\n",
    "\n",
    "plt.imshow(wc.generate(combined_fake_data))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split data into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['text'].to_list(), train_df['target'].to_list(),\n",
    "                                                    random_state=0)\n",
    "# Carry out preprocessing on text data\n",
    "words_train, words_test = preprocess_gensim(X_train), preprocess_gensim(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Extract Bag-of-Words (BoW)\n",
    "vectorizer = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "features_test = vectorizer.transform(words_test).toarray()\n",
    "\n",
    "# Create a vocabulary from the dataset\n",
    "vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# View random words in the vocabulary and confirm BoW representation of train and test data\n",
    "print(\"Vocabulary: {} words\".format(len(vocabulary)))\n",
    "print(\"Sample words: {}\".format(random.sample(list(vocabulary.keys()), 8)))\n",
    "print('\\n')\n",
    "print(features_train[0])\n",
    "print(features_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Zipf's law\n",
    "Zipf's law, named after the famous American linguist George Zipf, \n",
    "is an empirical law stating that given a large collection of documents, \n",
    "the frequency of any word is inversely proportional to its rank in the frequency table. \n",
    "So the most frequent word will occur about twice as often as the second most frequent word, \n",
    "three times as often as the third most frequent word, and so on. \n",
    "In the figure below we plot number of appearances of each word in our training set against its rank.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Find number of occurrences for each word in the training set\n",
    "word_freq = features_train.sum(axis=0)\n",
    "\n",
    "# Sort it in descending order\n",
    "sorted_word_freq = np.sort(word_freq)[::-1]\n",
    "\n",
    "# Plot \n",
    "plt.plot(sorted_word_freq)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.gca().set_yscale('log')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Number of occurrences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as pr\n",
    "\n",
    "# TODO: Normalize BoW features in training and test set\n",
    "features_train = pr.normalize(features_train, axis=0)\n",
    "features_test = pr.normalize(features_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# TODO: Train a Guassian Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "nb.fit(features_train, y_train)\n",
    "\n",
    "# Calculate the mean accuracy score on training and test sets\n",
    "print(\"[{}] Accuracy: train = {}, test = {}\".format(\n",
    "        nb.__class__.__name__,\n",
    "        nb.score(features_train, y_train),\n",
    "        nb.score(features_test, y_test)))\n",
    "\n",
    "'''\n",
    "Tree-based algorithms often work quite well on Bag-of-Words as their highly discontinuous and sparse nature \n",
    "is nicely matched by the structure of trees. \n",
    "As your next task, you will try to improve on the Naive Bayes classifier's performance by using \n",
    "scikit-learn's Gradient-Boosted Decision Tree classifer.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "logreg = LogisticRegressionCV(cv=5, scoring='accuracy', verbose=3, random_state=0, max_iter=1000)\n",
    "\n",
    "logreg.fit(features_train, y_train)\n",
    "\n",
    "print(\"[{}] Accuracy: train = {}, test = {}\".format(\n",
    "        logreg.__class__.__name__,\n",
    "        logreg.score(features_train, y_train),\n",
    "        logreg.score(features_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=5, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "gbc.fit(features_train, y_train)\n",
    "\n",
    "print('Accuracy of the GBM on training set: {:.3f}'.format(gbc.score(features_train, y_train)))\n",
    "print('Accuracy of the GBM on test set: {0:.3f}'.format(gbc.score(features_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=10)}\n",
    "cv_method = RepeatedStratifiedKFold(n_splits=2, \n",
    "                                    n_repeats=3, \n",
    "                                    random_state=0)\n",
    "\n",
    "gs_NB = GridSearchCV(estimator=nb, \n",
    "                     param_grid=params_NB, \n",
    "                     cv=cv_method,\n",
    "                     verbose=1, \n",
    "                     scoring='accuracy')\n",
    "\n",
    "\n",
    "gs_NB.fit(features_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs_NB.best_params_)\n",
    "print(gs_NB.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=1000, random_state=0)\n",
    "hyperparameters = dict(C=uniform(loc=0, scale=4), penalty=['l2', 'l1'])\n",
    "param_grid = {'C': [100, 10, 1.0, 0.1, 0.01]}\n",
    "k = RepeatedStratifiedKFold(n_splits=2, n_repeats=3, random_state=0)\n",
    "\n",
    "grid = GridSearchCV(logistic, param_grid=param_grid, cv=k, n_jobs=4, verbose=1)\n",
    "grid.fit(features_train, y_train)\n",
    "\n",
    "print('Best C:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the GridSearch on training set: {:.3f}'.format(grid.score(features_train, y_train)))\n",
    "print('Accuracy of the GridSearch on test set: {0:.3f}'.format(grid.score(features_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score   #Additional scklearn functions\n",
    "\n",
    "cv_score1 = cross_val_score(gbc, features_train, y_train, cv=3, scoring='roc_auc')\n",
    "cv_score2 = cross_val_score(nb, features_train, y_train, cv=3, scoring='roc_auc')\n",
    "cv_score3 = cross_val_score(logreg, features_train, y_train, cv=3, scoring='roc_auc')\n",
    "cv_score4 = cross_val_score(grid, features_train, y_train, cv=3, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_score1)\n",
    "print(cv_score2)\n",
    "print(cv_score3)\n",
    "print(cv_score4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "y_pred1 = nb.predict(features_test)\n",
    "y_pred2 = logreg.predict(features_test)\n",
    "y_pred3 = grid.predict(features_test)\n",
    "\n",
    "\n",
    "print(\"---Test Set Results---\")\n",
    "print(\"Accuracy with Gaussian: {}\".format(accuracy_score(y_test, y_pred1)))\n",
    "print(\"Accuracy with logreg: {}\".format(accuracy_score(y_test, y_pred2)))\n",
    "print(\"Accuracy with logreg: {}\".format(accuracy_score(y_test, y_pred3)))\n",
    "\n",
    "print(\"AUC Score with Gaussian: {}\".format(roc_auc_score(y_test, y_pred1)))\n",
    "print(\"AUC Score with Gaussian: {}\".format(roc_auc_score(y_test, y_pred2)))\n",
    "print(\"AUC Score with Gaussian: {}\".format(roc_auc_score(y_test, y_pred3)))\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred1))\n",
    "print(classification_report(y_test, y_pred2))\n",
    "print(classification_report(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_df['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = preprocess_gensim(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vectorizer.transform(test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = grid.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to RNNs\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(words_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(words_train)\n",
    "X_test = tokenizer.texts_to_sequences(words_test)\n",
    "\n",
    "vocabulary_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(words_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum number of words in a tweet\n",
    "\n",
    "max_length = 0\n",
    "for i,x in enumerate(words_train):\n",
    "    if len(words_train[i]) > max_length:\n",
    "        max_length = len(words_train[i])\n",
    "\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "# Set the maximum number of words per document (for both training and testing) by padding sequences \n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=max_length)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocabulary_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=max_length))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate the model\n",
    "model.fit(X_train, y_train, epochs=50, verbose=False, validation_data=(X_test, y_test), batch_size=10)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save the model\n",
    "cache_dir = os.path.join(\"cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "model_file = \"rnn_model.h5\"  # HDF5 file\n",
    "model.save(os.path.join(cache_dir, model_file))\n",
    "\n",
    "# Later you can load it using keras.models.load_model()\n",
    "#from keras.models import load_model\n",
    "#model = load_model(os.path.join(cache_dir, model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitf2b8979454eb492e8e911544bb13ab4b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
